{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd9f1ff-35ea-4c17-aaf4-c1af8276a449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Session Created Successfully!\n",
      "üöÄ Spark Version: 3.5.0\n",
      "üñ• Master: spark://spark-master:7077\n",
      "üìä Available Cores: 2\n",
      "üÜî Application ID: app-20250626013602-0001\n",
      "\n",
      "üîó Cluster Connection Status:\n",
      "   Workers connected: Check Spark UI at http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create Spark session with connection to your cluster\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Analysis - Day 11\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbose output\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"‚úÖ Spark Session Created Successfully!\")\n",
    "print(f\"üöÄ Spark Version: {spark.version}\")\n",
    "print(f\"üñ• Master: {spark.sparkContext.master}\")\n",
    "print(f\"üìä Available Cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"üÜî Application ID: {spark.sparkContext.applicationId}\")\n",
    "\n",
    "# Check connection to cluster\n",
    "print(f\"\\nüîó Cluster Connection Status:\")\n",
    "print(f\"   Workers connected: Check Spark UI at http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d9b0ab-5125-4f23-a814-462bfefd8d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading NYC Taxi Data...\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load and Explore Data\n",
    "print(\"üìÇ Loading NYC Taxi Data...\")\n",
    "\n",
    "# Load data with schema inference\n",
    "taxi_df = spark.read.csv(\"/home/jovyan/data/nyc_taxi_data.csv\",\n",
    "                        header=True,\n",
    "                        inferSchema=True)\n",
    "\n",
    "print(f\"üìä Dataset Shape: {taxi_df.count()} rows x {len(taxi_df.columns)} columns\")\n",
    "\n",
    "print(\"\\nüîç Schema Information:\")\n",
    "taxi_df.printSchema()\n",
    "\n",
    "print(\"\\nüìã Sample Data (First 5 rows):\")\n",
    "taxi_df.show(5, truncate=False)\n",
    "\n",
    "print(\"\\nüìà Column Names:\")\n",
    "for i, column in enumerate(taxi_df.columns):\n",
    "    print(f\"  {i+1:2d}. {column}\")\n",
    "\n",
    "print(f\"\\nüéØ Success! Loaded {taxi_df.count():,} taxi trip records into Spark DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f2418-c19a-4b65-90b2-5a9778a7d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is taking more than 20 minutes to run previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17254b72-633b-44ad-bc73-b673f97d5c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous session to stop\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "    print(\"‚úÖ Stopped previous Spark session\")\n",
    "except:\n",
    "    print(\"No previous session to stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8b6c990-e21d-4141-b86e-ab0fd27df987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking file accessibility...\n",
      "Current directory: /home/jovyan\n",
      "Files in /home/jovyan/data/: ['generate_sample_data.py', 'nyc_taxi_data.csv']\n",
      "‚úÖ File found: /home/jovyan/data/nyc_taxi_data.csv\n",
      "üìä File size: 1,253,689 bytes (1.2 MB)\n",
      "‚úÖ Pandas can read it: 5 rows, 18 columns\n",
      "First few rows:\n",
      "   vendor_id tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0          1  2024-01-06 09:42:36   2024-01-06 09:44:36                2   \n",
      "1          1  2024-01-02 10:42:58   2024-01-02 10:48:58                2   \n",
      "2          2  2024-01-22 19:21:59   2024-01-22 19:59:59                1   \n",
      "3          2  2024-01-05 23:05:00   2024-01-05 23:11:00                2   \n",
      "4          2  2024-01-04 19:48:31   2024-01-04 20:24:31                1   \n",
      "\n",
      "   trip_distance  pickup_longitude  pickup_latitude  rate_code_id  \\\n",
      "0          14.84        -73.827535        40.698978             1   \n",
      "1          13.03        -73.984409        40.751071             1   \n",
      "2           8.51        -73.966543        40.823860             1   \n",
      "3           0.96        -73.936022        40.721796             1   \n",
      "4           3.93        -73.962047        40.775728             1   \n",
      "\n",
      "  store_and_fwd_flag  dropoff_longitude  dropoff_latitude  payment_type  \\\n",
      "0                  N         -73.841953         40.683078             1   \n",
      "1                  N         -74.003348         40.739025             2   \n",
      "2                  N         -73.956191         40.810246             1   \n",
      "3                  N         -73.942264         40.712377             1   \n",
      "4                  N         -73.946629         40.770193             1   \n",
      "\n",
      "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  total_amount  \n",
      "0        40.60    0.0      0.5        5.99           0.0         47.09  \n",
      "1        38.07    0.0      0.5        0.00           0.0         38.57  \n",
      "2        42.77    0.0      0.5        2.30           0.0         45.57  \n",
      "3         7.90    0.0      0.5        1.92           0.0         10.32  \n",
      "4        30.33    0.0      0.5        5.01           0.0         35.84  \n"
     ]
    }
   ],
   "source": [
    "# Check if our data file is accessible\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üîç Checking file accessibility...\")\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# List files in data directory\n",
    "try:\n",
    "    files = os.listdir('/home/jovyan/data/')\n",
    "    print(f\"Files in /home/jovyan/data/: {files}\")\n",
    "    \n",
    "    # Check file size\n",
    "    if 'nyc_taxi_data.csv' in files:\n",
    "        file_path = '/home/jovyan/data/nyc_taxi_data.csv'\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        print(f\"‚úÖ File found: {file_path}\")\n",
    "        print(f\"üìä File size: {file_size:,} bytes ({file_size/1024/1024:.1f} MB)\")\n",
    "        \n",
    "        # Quick check with pandas\n",
    "        df_test = pd.read_csv(file_path, nrows=5)\n",
    "        print(f\"‚úÖ Pandas can read it: {len(df_test)} rows, {len(df_test.columns)} columns\")\n",
    "        print(\"First few rows:\")\n",
    "        print(df_test.head())\n",
    "    else:\n",
    "        print(\"‚ùå nyc_taxi_data.csv not found!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking files: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c81f1f-4366-4cf5-85b3-49d88b61879c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating Spark session with simpler config...\n",
      "‚úÖ Spark Session Created!\n",
      "üöÄ Spark Version: 3.5.0\n",
      "üñ• Master: local[2]\n",
      "üìä Available Cores: 2\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Simple Spark Session (Troubleshooting Version)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"üöÄ Creating Spark session with simpler config...\")\n",
    "\n",
    "# Create Spark session with local mode first (for troubleshooting)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Analysis - Troubleshooting\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # Reduce noise\n",
    "\n",
    "print(\"‚úÖ Spark Session Created!\")\n",
    "print(f\"üöÄ Spark Version: {spark.version}\")\n",
    "print(f\"üñ• Master: {spark.sparkContext.master}\")\n",
    "print(f\"üìä Available Cores: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c3bb43-bda5-42d1-870f-b4cdb33c9579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading data with local Spark...\n",
      "‚úÖ Data loaded!\n",
      "Row count: 10000\n",
      "Columns: 18\n",
      "+---------+--------------------+---------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "|vendor_id|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|rate_code_id|store_and_fwd_flag|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+---------+--------------------+---------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "|        1| 2024-01-06 09:42:36|  2024-01-06 09:44:36|              2|        14.84|      -73.827535|      40.698978|           1|                 N|       -73.841953|       40.683078|           1|       40.6|  0.0|    0.5|      5.99|         0.0|       47.09|\n",
      "|        1| 2024-01-02 10:42:58|  2024-01-02 10:48:58|              2|        13.03|      -73.984409|      40.751071|           1|                 N|       -74.003348|       40.739025|           2|      38.07|  0.0|    0.5|       0.0|         0.0|       38.57|\n",
      "|        2| 2024-01-22 19:21:59|  2024-01-22 19:59:59|              1|         8.51|      -73.966543|       40.82386|           1|                 N|       -73.956191|       40.810246|           1|      42.77|  0.0|    0.5|       2.3|         0.0|       45.57|\n",
      "+---------+--------------------+---------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Simple Data Loading Test\n",
    "print(\"üìÇ Loading data with local Spark...\")\n",
    "\n",
    "try:\n",
    "    # Very simple loading\n",
    "    taxi_df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(\"/home/jovyan/data/nyc_taxi_data.csv\")\n",
    "    \n",
    "    print(\"‚úÖ Data loaded!\")\n",
    "    print(f\"Row count: {taxi_df.count()}\")\n",
    "    print(f\"Columns: {len(taxi_df.columns)}\")\n",
    "    \n",
    "    # Show first 3 rows\n",
    "    taxi_df.show(3)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe1f9def-0acb-49b8-978f-4f0f14fc0517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Data Quality Assessment:\n",
      "\n",
      "‚ùì Missing Values per Column:\n",
      "+---------+--------------------+---------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "|vendor_id|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|rate_code_id|store_and_fwd_flag|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+---------+--------------------+---------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "|        0|                   0|                    0|              0|            0|               0|              0|           0|                 0|                0|               0|           0|          0|    0|      0|         0|           0|           0|\n",
      "+---------+--------------------+---------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "\n",
      "\n",
      "üìà Numerical Columns Statistics:\n",
      "+-------+------------------+--------------------+---------------------+-----------------+-----------------+-------------------+-------------------+------------+------------------+-------------------+-------------------+------------------+------------------+-----+-------+------------------+------------+------------------+\n",
      "|summary|         vendor_id|tpep_pickup_datetime|tpep_dropoff_datetime|  passenger_count|    trip_distance|   pickup_longitude|    pickup_latitude|rate_code_id|store_and_fwd_flag|  dropoff_longitude|   dropoff_latitude|      payment_type|       fare_amount|extra|mta_tax|        tip_amount|tolls_amount|      total_amount|\n",
      "+-------+------------------+--------------------+---------------------+-----------------+-----------------+-------------------+-------------------+------------+------------------+-------------------+-------------------+------------------+------------------+-----+-------+------------------+------------+------------------+\n",
      "|  count|             10000|               10000|                10000|            10000|            10000|              10000|              10000|       10000|             10000|              10000|              10000|             10000|             10000|10000|  10000|             10000|       10000|             10000|\n",
      "|   mean|            1.4916|                NULL|                 NULL|           2.0115|10.17212199999997|  -73.9002803078999|  40.75043671140014|         1.0|              NULL| -73.90032195650019| 40.750533123799975|              1.25|43.230005999999875|  0.0|    0.5|4.0338960000000075|         0.0| 47.76390199999996|\n",
      "| stddev|0.4999544333672262|                NULL|                 NULL|1.069711259274119|5.711291076603632|0.08673974076472939|0.05755070294535415|         0.0|              NULL|0.08757643256480697|0.05874616947926592|0.4330343541512467|16.800345927459087|  0.0|    0.0| 4.128931257422052|         0.0|18.735807003190484|\n",
      "|    min|                 1| 2024-01-01 00:00:27|  2024-01-01 00:04:33|                1|              0.1|          -73.75007|           40.65003|           1|                 N|         -73.731878|          40.630564|                 1|              10.0|  0.0|    0.5|               0.0|         0.0|             10.05|\n",
      "|    max|                 2| 2024-01-31 23:48:41|  2024-02-01 00:24:25|                4|             9.99|         -74.049974|          40.849994|           1|                 N|          -74.06891|          40.868854|                 2|              9.97|  0.0|    0.5|              9.99|         0.0|             99.53|\n",
      "+-------+------------------+--------------------+---------------------+-----------------+-----------------+-------------------+-------------------+------------+------------------+-------------------+-------------------+------------------+------------------+-----+-------+------------------+------------+------------------+\n",
      "\n",
      "\n",
      "üóì Date Range:\n",
      "+-------------------+-------------------+-----------+\n",
      "|    earliest_pickup|      latest_pickup|total_trips|\n",
      "+-------------------+-------------------+-----------+\n",
      "|2024-01-01 00:00:27|2024-01-31 23:48:41|      10000|\n",
      "+-------------------+-------------------+-----------+\n",
      "\n",
      "\n",
      "üí≥ Payment Type Distribution:\n",
      "+------------+-----+\n",
      "|payment_type|count|\n",
      "+------------+-----+\n",
      "|           1| 7500|\n",
      "|           2| 2500|\n",
      "+------------+-----+\n",
      "\n",
      "\n",
      "üë• Passenger Count Distribution:\n",
      "+---------------+-----+\n",
      "|passenger_count|count|\n",
      "+---------------+-----+\n",
      "|              1| 4220|\n",
      "|              2| 2889|\n",
      "|              3| 1447|\n",
      "|              4| 1444|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Quality Analysis\n",
    "print(\"üîç Data Quality Assessment:\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n‚ùì Missing Values per Column:\")\n",
    "missing_counts = taxi_df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in taxi_df.columns\n",
    "])\n",
    "missing_counts.show()\n",
    "\n",
    "# Basic statistics for numerical columns\n",
    "print(\"\\nüìà Numerical Columns Statistics:\")\n",
    "taxi_df.describe().show()\n",
    "\n",
    "# Check data ranges\n",
    "print(\"\\nüóì Date Range:\")\n",
    "taxi_df.select(\n",
    "    min(\"tpep_pickup_datetime\").alias(\"earliest_pickup\"),\n",
    "    max(\"tpep_pickup_datetime\").alias(\"latest_pickup\"),\n",
    "    count(\"*\").alias(\"total_trips\")\n",
    ").show()\n",
    "\n",
    "# Payment type distribution\n",
    "print(\"\\nüí≥ Payment Type Distribution:\")\n",
    "taxi_df.groupBy(\"payment_type\").count().orderBy(\"payment_type\").show()\n",
    "\n",
    "# Passenger count distribution\n",
    "print(\"\\nüë• Passenger Count Distribution:\")\n",
    "taxi_df.groupBy(\"passenger_count\").count().orderBy(\"passenger_count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18ec9101-3bb9-4d6e-a205-691bd299be13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Data Cleaning and Feature Engineering...\n",
      "üìä Original records: 10,000\n",
      "üìä After cleaning: 9,573\n",
      "üìä Removed: 427 records\n",
      "üìä Final dataset: 9,410 records\n",
      "üíæ DataFrame cached for better performance\n",
      "\n",
      "‚úÖ Data transformation completed!\n",
      "üîç New columns: pickup_hour, pickup_day_of_week, trip_duration_minutes, tip_percentage, fare_per_mile\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Data Transformations and Feature Engineering\n",
    "print(\"üßπ Data Cleaning and Feature Engineering...\")\n",
    "\n",
    "# First, let's properly cast the datetime columns\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "# Convert string datetime to proper timestamp type\n",
    "taxi_df_typed = taxi_df.withColumn(\n",
    "    \"pickup_datetime\", to_timestamp(\"tpep_pickup_datetime\", \"yyyy-MM-dd HH:mm:ss\")\n",
    ").withColumn(\n",
    "    \"dropoff_datetime\", to_timestamp(\"tpep_dropoff_datetime\", \"yyyy-MM-dd HH:mm:ss\")\n",
    ")\n",
    "\n",
    "# Clean the data (remove invalid trips)\n",
    "cleaned_taxi_df = taxi_df_typed.filter(\n",
    "    (col(\"trip_distance\") > 0) &\n",
    "    (col(\"fare_amount\") > 0) &\n",
    "    (col(\"total_amount\") > 0) &\n",
    "    (col(\"passenger_count\") > 0) &\n",
    "    (col(\"passenger_count\") <= 6) &\n",
    "    (col(\"trip_distance\") < 50) &\n",
    "    (col(\"fare_amount\") < 200) &\n",
    "    (col(\"total_amount\") < 300)\n",
    ")\n",
    "\n",
    "print(f\"üìä Original records: {taxi_df.count():,}\")\n",
    "print(f\"üìä After cleaning: {cleaned_taxi_df.count():,}\")\n",
    "print(f\"üìä Removed: {taxi_df.count() - cleaned_taxi_df.count():,} records\")\n",
    "\n",
    "# Feature engineering\n",
    "enriched_taxi_df = cleaned_taxi_df.withColumn(\n",
    "    \"pickup_hour\", hour(\"pickup_datetime\")\n",
    ").withColumn(\n",
    "    \"pickup_day_of_week\", dayofweek(\"pickup_datetime\") \n",
    ").withColumn(\n",
    "    \"trip_duration_minutes\",\n",
    "    (unix_timestamp(\"dropoff_datetime\") - unix_timestamp(\"pickup_datetime\")) / 60\n",
    ").withColumn(\n",
    "    \"tip_percentage\",\n",
    "    when(col(\"fare_amount\") > 0, (col(\"tip_amount\") / col(\"fare_amount\")) * 100).otherwise(0)\n",
    ").withColumn(\n",
    "    \"fare_per_mile\",\n",
    "    when(col(\"trip_distance\") > 0, col(\"fare_amount\") / col(\"trip_distance\")).otherwise(0)\n",
    ")\n",
    "\n",
    "# Final filtering\n",
    "final_taxi_df = enriched_taxi_df.filter(\n",
    "    (col(\"trip_duration_minutes\") > 1) &\n",
    "    (col(\"trip_duration_minutes\") < 120) &\n",
    "    (col(\"fare_per_mile\") > 0) &\n",
    "    (col(\"fare_per_mile\") < 50)\n",
    ")\n",
    "\n",
    "print(f\"üìä Final dataset: {final_taxi_df.count():,} records\")\n",
    "\n",
    "# Cache for better performance\n",
    "final_taxi_df.cache()\n",
    "print(\"üíæ DataFrame cached for better performance\")\n",
    "\n",
    "print(\"\\n‚úÖ Data transformation completed!\")\n",
    "print(\"üîç New columns: pickup_hour, pickup_day_of_week, trip_duration_minutes, tip_percentage, fare_per_mile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c40d19-ebd1-4d11-b1d6-e72a1f9d4aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Business Analytics with Spark SQL\n",
      "\n",
      "‚è∞ Peak Hours Analysis:\n",
      "+-----------+----------+------------+--------+-----------+\n",
      "|pickup_hour|trip_count|avg_distance|avg_fare|avg_tip_pct|\n",
      "+-----------+----------+------------+--------+-----------+\n",
      "|          0|       395|        10.7|   49.01|       9.37|\n",
      "|          1|       387|        10.5|   48.54|       9.15|\n",
      "|          2|       377|        10.8|   49.25|       9.37|\n",
      "|          3|       400|       10.29|   48.55|       9.88|\n",
      "|          4|       396|       10.69|   51.03|       9.04|\n",
      "|          5|       370|        10.5|   48.17|       8.91|\n",
      "|          6|       414|       10.78|   50.11|       9.29|\n",
      "|          7|       370|       10.01|   47.67|       9.41|\n",
      "|          8|       392|       10.96|   51.03|       9.65|\n",
      "|          9|       403|       10.59|   49.35|       9.07|\n",
      "|         10|       384|       10.81|    49.8|      10.12|\n",
      "|         11|       402|       10.46|    49.1|       8.91|\n",
      "|         12|       380|       10.77|   49.39|       8.66|\n",
      "|         13|       366|       10.04|   47.13|       9.31|\n",
      "|         14|       369|       10.64|   49.53|       9.28|\n",
      "|         15|       373|       11.07|   51.57|        9.8|\n",
      "|         16|       424|       10.86|   49.28|       9.31|\n",
      "|         17|       402|        10.9|   49.92|       9.58|\n",
      "|         18|       395|        10.5|   49.05|       9.88|\n",
      "|         19|       373|       10.25|   47.52|       9.48|\n",
      "|         20|       411|       10.42|   49.12|       9.42|\n",
      "|         21|       402|        10.6|   49.21|       9.02|\n",
      "|         22|       403|        10.8|    50.1|       9.78|\n",
      "|         23|       422|       10.42|   47.52|       9.16|\n",
      "+-----------+----------+------------+--------+-----------+\n",
      "\n",
      "\n",
      "üìÖ Day of Week Analysis:\n",
      "+---------+----------+------------+---------+\n",
      "| day_name|trip_count|avg_distance|avg_total|\n",
      "+---------+----------+------------+---------+\n",
      "|   Sunday|      1200|       10.58|    49.38|\n",
      "|   Monday|      1501|        10.6|    48.63|\n",
      "|  Tuesday|      1468|       10.57|    49.26|\n",
      "|Wednesday|      1528|       10.38|    48.78|\n",
      "| Thursday|      1220|       10.78|    49.35|\n",
      "|   Friday|      1262|       10.61|    49.79|\n",
      "| Saturday|      1231|       10.75|    49.52|\n",
      "+---------+----------+------------+---------+\n",
      "\n",
      "\n",
      "üìè Distance Analysis:\n",
      "+------------------+----------+----------------+--------+\n",
      "| distance_category|trip_count|avg_duration_min|avg_fare|\n",
      "+------------------+----------+----------------+--------+\n",
      "|     Short (‚â§1 mi)|       433|            30.5|   24.07|\n",
      "|   Medium (1-5 mi)|      1986|            30.5|   30.83|\n",
      "|    Long (5-10 mi)|      2465|            31.3|   43.62|\n",
      "|Very Long (>10 mi)|      4526|            31.3|   62.73|\n",
      "+------------------+----------+----------------+--------+\n",
      "\n",
      "\n",
      "üéØ Key Insights Generated!\n",
      "üìä You've successfully processed 10,000+ records with Spark!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Business Analytics with Spark SQL\n",
    "print(\"üéØ Business Analytics with Spark SQL\")\n",
    "\n",
    "# Register DataFrame as SQL table\n",
    "final_taxi_df.createOrReplaceTempView(\"taxi_trips\")\n",
    "\n",
    "# Analysis 1: Peak hours analysis\n",
    "print(\"\\n‚è∞ Peak Hours Analysis:\")\n",
    "peak_hours = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        pickup_hour,\n",
    "        COUNT(*) as trip_count,\n",
    "        ROUND(AVG(trip_distance), 2) as avg_distance,\n",
    "        ROUND(AVG(total_amount), 2) as avg_fare,\n",
    "        ROUND(AVG(tip_percentage), 2) as avg_tip_pct\n",
    "    FROM taxi_trips \n",
    "    GROUP BY pickup_hour \n",
    "    ORDER BY pickup_hour\n",
    "\"\"\")\n",
    "peak_hours.show(24)\n",
    "\n",
    "# Analysis 2: Day of week patterns  \n",
    "print(\"\\nüìÖ Day of Week Analysis:\")\n",
    "dow_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE pickup_day_of_week\n",
    "            WHEN 1 THEN 'Sunday'\n",
    "            WHEN 2 THEN 'Monday' \n",
    "            WHEN 3 THEN 'Tuesday'\n",
    "            WHEN 4 THEN 'Wednesday'\n",
    "            WHEN 5 THEN 'Thursday'\n",
    "            WHEN 6 THEN 'Friday'\n",
    "            WHEN 7 THEN 'Saturday'\n",
    "        END as day_name,\n",
    "        COUNT(*) as trip_count,\n",
    "        ROUND(AVG(trip_distance), 2) as avg_distance,\n",
    "        ROUND(AVG(total_amount), 2) as avg_total\n",
    "    FROM taxi_trips \n",
    "    GROUP BY pickup_day_of_week \n",
    "    ORDER BY pickup_day_of_week\n",
    "\"\"\")\n",
    "dow_analysis.show()\n",
    "\n",
    "# Analysis 3: Distance categories\n",
    "print(\"\\nüìè Distance Analysis:\")\n",
    "distance_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN trip_distance <= 1 THEN 'Short (‚â§1 mi)'\n",
    "            WHEN trip_distance <= 5 THEN 'Medium (1-5 mi)' \n",
    "            WHEN trip_distance <= 10 THEN 'Long (5-10 mi)'\n",
    "            ELSE 'Very Long (>10 mi)'\n",
    "        END as distance_category,\n",
    "        COUNT(*) as trip_count,\n",
    "        ROUND(AVG(trip_duration_minutes), 1) as avg_duration_min,\n",
    "        ROUND(AVG(total_amount), 2) as avg_fare\n",
    "    FROM taxi_trips\n",
    "    GROUP BY \n",
    "        CASE \n",
    "            WHEN trip_distance <= 1 THEN 'Short (‚â§1 mi)'\n",
    "            WHEN trip_distance <= 5 THEN 'Medium (1-5 mi)' \n",
    "            WHEN trip_distance <= 10 THEN 'Long (5-10 mi)'\n",
    "            ELSE 'Very Long (>10 mi)'\n",
    "        END\n",
    "    ORDER BY avg_duration_min\n",
    "\"\"\")\n",
    "distance_analysis.show()\n",
    "\n",
    "print(\"\\nüéØ Key Insights Generated!\")\n",
    "print(\"üìä You've successfully processed 10,000+ records with Spark!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "614d2d3b-3aa6-4a2f-84fc-244fb0ebdedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üó∫ Geographic Analysis:\n",
      "üîù Top Pickup Locations:\n",
      "+------------------+------------------+------------+-----------------+--------+\n",
      "|pickup_lon_rounded|pickup_lat_rounded|pickup_count|avg_trip_distance|avg_fare|\n",
      "+------------------+------------------+------------+-----------------+--------+\n",
      "+------------------+------------------+------------+-----------------+--------+\n",
      "\n",
      "\n",
      "‚úà Airport Trip Analysis:\n",
      "+-----------+----------+------------------+------------------+------------------+\n",
      "|   location|trip_count|      avg_distance|          avg_fare|      avg_duration|\n",
      "+-----------+----------+------------------+------------------+------------------+\n",
      "|JFK Airport|        72|10.072083333333335|46.365555555555574|            28.625|\n",
      "|LGA Airport|       221|10.614615384615378|49.093755656108605|30.176470588235293|\n",
      "+-----------+----------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Geographic hot spots analysis\n",
    "print(\"\\nüó∫ Geographic Analysis:\")\n",
    "\n",
    "# Popular pickup locations (rounded by coordinate)\n",
    "pickup_hotspots = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        ROUND(pickup_longitude, 3) AS pickup_lon_rounded,\n",
    "        ROUND(pickup_latitude, 3) AS pickup_lat_rounded,\n",
    "        COUNT(*) AS pickup_count,\n",
    "        AVG(trip_distance) AS avg_trip_distance,\n",
    "        AVG(total_amount) AS avg_fare\n",
    "    FROM taxi_trips\n",
    "    WHERE pickup_longitude BETWEEN -74.05 AND -73.75\n",
    "      AND pickup_latitude BETWEEN 40.65 AND 40.85\n",
    "    GROUP BY pickup_lon_rounded, pickup_lat_rounded\n",
    "    HAVING pickup_count >= 100\n",
    "    ORDER BY pickup_count DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "print(\"üîù Top Pickup Locations:\")\n",
    "pickup_hotspots.show()\n",
    "\n",
    "# Airport trips analysis\n",
    "airport_trips = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        'JFK Airport' AS location,\n",
    "        COUNT(*) AS trip_count,\n",
    "        AVG(trip_distance) AS avg_distance,\n",
    "        AVG(total_amount) AS avg_fare,\n",
    "        AVG(trip_duration_minutes) AS avg_duration\n",
    "    FROM taxi_trips\n",
    "    WHERE (pickup_longitude BETWEEN -73.79 AND -73.76 AND pickup_latitude BETWEEN 40.64 AND 40.66)\n",
    "       OR (dropoff_longitude BETWEEN -73.79 AND -73.76 AND dropoff_latitude BETWEEN 40.64 AND 40.66)\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'LGA Airport' AS location,\n",
    "        COUNT(*) AS trip_count,\n",
    "        AVG(trip_distance) AS avg_distance,\n",
    "        AVG(total_amount) AS avg_fare,\n",
    "        AVG(trip_duration_minutes) AS avg_duration\n",
    "    FROM taxi_trips\n",
    "    WHERE (pickup_longitude BETWEEN -73.89 AND -73.85 AND pickup_latitude BETWEEN 40.76 AND 40.78)\n",
    "       OR (dropoff_longitude BETWEEN -73.89 AND -73.85 AND dropoff_latitude BETWEEN 40.76 AND 40.78)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úà Airport Trip Analysis:\")\n",
    "airport_trips.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30829a8a-9424-4e23-b488-c9efaf8e8655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Performance Optimization Techniques\n",
      "\n",
      "üì¶ Current Partitions: 1\n",
      "üìä Records per Partition:\n",
      "+------------+---------------------+\n",
      "|partition_id|records_per_partition|\n",
      "+------------+---------------------+\n",
      "|           0|                 9410|\n",
      "+------------+---------------------+\n",
      "\n",
      "\n",
      "üîÑ Optimizing Partitions...\n",
      "üì¶ New Partitions: 8\n",
      "\n",
      "üì° Broadcast Join Example:\n",
      "‚úÖ Payment method mapping applied with broadcast join\n",
      "\n",
      "üìà Advanced Window Functions:\n",
      "‚úÖ Window functions applied for running averages and rankings\n"
     ]
    }
   ],
   "source": [
    "# Performance optimization techniques\n",
    "print(\"üöÄ Performance Optimization Techniques\")\n",
    "\n",
    "# 1. Partitioning analysis\n",
    "print(f\"\\nüì¶ Current Partitions: {final_taxi_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Check partition sizes\n",
    "partition_info = spark.sql(\"\"\"\n",
    "    SELECT spark_partition_id() AS partition_id, COUNT(*) AS records_per_partition\n",
    "    FROM taxi_trips\n",
    "    GROUP BY spark_partition_id()\n",
    "    ORDER BY spark_partition_id()\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìä Records per Partition:\")\n",
    "partition_info.show()\n",
    "\n",
    "# 2. Repartitioning for better performance\n",
    "print(\"\\nüîÑ Optimizing Partitions...\")\n",
    "# Repartition based on pickup_hour for time-based analysis\n",
    "optimized_df = final_taxi_df.repartition(8, \"pickup_hour\").cache()\n",
    "\n",
    "print(f\"üì¶ New Partitions: {optimized_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# 3. Broadcast join optimization example\n",
    "print(\"\\nüì° Broadcast Join Example:\")\n",
    "\n",
    "# Create a small lookup table for payment types\n",
    "payment_types = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"Credit Card\"),\n",
    "        (2, \"Cash\"),\n",
    "        (3, \"No Charge\"),\n",
    "        (4, \"Dispute\"),\n",
    "        (5, \"Unknown\"),\n",
    "        (6, \"Voided Trip\")\n",
    "    ],\n",
    "    [\"payment_type\", \"payment_method\"]\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "enriched_df = optimized_df.join(\n",
    "    broadcast(payment_types),\n",
    "    on=\"payment_type\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Payment method mapping applied with broadcast join\")\n",
    "\n",
    "# 4. Aggregation with window functions\n",
    "print(\"\\nüìà Advanced Window Functions:\")\n",
    "\n",
    "from pyspark.sql.functions import avg, row_number, desc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Calculate running averages and rankings\n",
    "window_spec = Window.partitionBy(\"pickup_hour\").orderBy(\"tpep_pickup_datetime\")\n",
    "\n",
    "windowed_analysis = (\n",
    "    optimized_df\n",
    "    .withColumn(\n",
    "        \"running_avg_fare\",\n",
    "        avg(\"total_amount\").over(window_spec.rowsBetween(-100, 0))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"trip_rank_in_hour\",\n",
    "        row_number().over(Window.partitionBy(\"pickup_hour\").orderBy(desc(\"total_amount\")))\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Window functions applied for running averages and rankings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6a1fb5-bf0c-4238-9ff0-468fbf9e3547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
