{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd9f1ff-35ea-4c17-aaf4-c1af8276a449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Session Created Successfully!\n",
      "🚀 Spark Version: 3.5.0\n",
      "🖥 Master: spark://spark-master:7077\n",
      "📊 Available Cores: 2\n",
      "🆔 Application ID: app-20250626013602-0001\n",
      "\n",
      "🔗 Cluster Connection Status:\n",
      "   Workers connected: Check Spark UI at http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create Spark session with connection to your cluster\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Analysis - Day 11\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbose output\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"✅ Spark Session Created Successfully!\")\n",
    "print(f\"🚀 Spark Version: {spark.version}\")\n",
    "print(f\"🖥 Master: {spark.sparkContext.master}\")\n",
    "print(f\"📊 Available Cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"🆔 Application ID: {spark.sparkContext.applicationId}\")\n",
    "\n",
    "# Check connection to cluster\n",
    "print(f\"\\n🔗 Cluster Connection Status:\")\n",
    "print(f\"   Workers connected: Check Spark UI at http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d9b0ab-5125-4f23-a814-462bfefd8d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading NYC Taxi Data...\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load and Explore Data\n",
    "print(\"📂 Loading NYC Taxi Data...\")\n",
    "\n",
    "# Load data with schema inference\n",
    "taxi_df = spark.read.csv(\"/home/jovyan/data/nyc_taxi_data.csv\",\n",
    "                        header=True,\n",
    "                        inferSchema=True)\n",
    "\n",
    "print(f\"📊 Dataset Shape: {taxi_df.count()} rows x {len(taxi_df.columns)} columns\")\n",
    "\n",
    "print(\"\\n🔍 Schema Information:\")\n",
    "taxi_df.printSchema()\n",
    "\n",
    "print(\"\\n📋 Sample Data (First 5 rows):\")\n",
    "taxi_df.show(5, truncate=False)\n",
    "\n",
    "print(\"\\n📈 Column Names:\")\n",
    "for i, column in enumerate(taxi_df.columns):\n",
    "    print(f\"  {i+1:2d}. {column}\")\n",
    "\n",
    "print(f\"\\n🎯 Success! Loaded {taxi_df.count():,} taxi trip records into Spark DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f2418-c19a-4b65-90b2-5a9778a7d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is taking more than 20 minutes to run previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17254b72-633b-44ad-bc73-b673f97d5c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous session to stop\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "    print(\"✅ Stopped previous Spark session\")\n",
    "except:\n",
    "    print(\"No previous session to stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8b6c990-e21d-4141-b86e-ab0fd27df987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking file accessibility...\n",
      "Current directory: /home/jovyan\n",
      "Files in /home/jovyan/data/: ['generate_sample_data.py', 'nyc_taxi_data.csv']\n",
      "✅ File found: /home/jovyan/data/nyc_taxi_data.csv\n",
      "📊 File size: 1,253,689 bytes (1.2 MB)\n",
      "✅ Pandas can read it: 5 rows, 18 columns\n",
      "First few rows:\n",
      "   vendor_id tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0          1  2024-01-06 09:42:36   2024-01-06 09:44:36                2   \n",
      "1          1  2024-01-02 10:42:58   2024-01-02 10:48:58                2   \n",
      "2          2  2024-01-22 19:21:59   2024-01-22 19:59:59                1   \n",
      "3          2  2024-01-05 23:05:00   2024-01-05 23:11:00                2   \n",
      "4          2  2024-01-04 19:48:31   2024-01-04 20:24:31                1   \n",
      "\n",
      "   trip_distance  pickup_longitude  pickup_latitude  rate_code_id  \\\n",
      "0          14.84        -73.827535        40.698978             1   \n",
      "1          13.03        -73.984409        40.751071             1   \n",
      "2           8.51        -73.966543        40.823860             1   \n",
      "3           0.96        -73.936022        40.721796             1   \n",
      "4           3.93        -73.962047        40.775728             1   \n",
      "\n",
      "  store_and_fwd_flag  dropoff_longitude  dropoff_latitude  payment_type  \\\n",
      "0                  N         -73.841953         40.683078             1   \n",
      "1                  N         -74.003348         40.739025             2   \n",
      "2                  N         -73.956191         40.810246             1   \n",
      "3                  N         -73.942264         40.712377             1   \n",
      "4                  N         -73.946629         40.770193             1   \n",
      "\n",
      "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  total_amount  \n",
      "0        40.60    0.0      0.5        5.99           0.0         47.09  \n",
      "1        38.07    0.0      0.5        0.00           0.0         38.57  \n",
      "2        42.77    0.0      0.5        2.30           0.0         45.57  \n",
      "3         7.90    0.0      0.5        1.92           0.0         10.32  \n",
      "4        30.33    0.0      0.5        5.01           0.0         35.84  \n"
     ]
    }
   ],
   "source": [
    "# Check if our data file is accessible\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "print(\"🔍 Checking file accessibility...\")\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# List files in data directory\n",
    "try:\n",
    "    files = os.listdir('/home/jovyan/data/')\n",
    "    print(f\"Files in /home/jovyan/data/: {files}\")\n",
    "    \n",
    "    # Check file size\n",
    "    if 'nyc_taxi_data.csv' in files:\n",
    "        file_path = '/home/jovyan/data/nyc_taxi_data.csv'\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        print(f\"✅ File found: {file_path}\")\n",
    "        print(f\"📊 File size: {file_size:,} bytes ({file_size/1024/1024:.1f} MB)\")\n",
    "        \n",
    "        # Quick check with pandas\n",
    "        df_test = pd.read_csv(file_path, nrows=5)\n",
    "        print(f\"✅ Pandas can read it: {len(df_test)} rows, {len(df_test.columns)} columns\")\n",
    "        print(\"First few rows:\")\n",
    "        print(df_test.head())\n",
    "    else:\n",
    "        print(\"❌ nyc_taxi_data.csv not found!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error checking files: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c81f1f-4366-4cf5-85b3-49d88b61879c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Creating Spark session with simpler config...\n",
      "✅ Spark Session Created!\n",
      "🚀 Spark Version: 3.5.0\n",
      "🖥 Master: local[2]\n",
      "📊 Available Cores: 2\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Simple Spark Session (Troubleshooting Version)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"🚀 Creating Spark session with simpler config...\")\n",
    "\n",
    "# Create Spark session with local mode first (for troubleshooting)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Analysis - Troubleshooting\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # Reduce noise\n",
    "\n",
    "print(\"✅ Spark Session Created!\")\n",
    "print(f\"🚀 Spark Version: {spark.version}\")\n",
    "print(f\"🖥 Master: {spark.sparkContext.master}\")\n",
    "print(f\"📊 Available Cores: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c3bb43-bda5-42d1-870f-b4cdb33c9579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading data with local Spark...\n",
      "✅ Data loaded!\n",
      "Row count: 10000\n",
      "Columns: 18\n",
      "+---------+--------------------+---------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "|vendor_id|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|rate_code_id|store_and_fwd_flag|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+---------+--------------------+---------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "|        1| 2024-01-06 09:42:36|  2024-01-06 09:44:36|              2|        14.84|      -73.827535|      40.698978|           1|                 N|       -73.841953|       40.683078|           1|       40.6|  0.0|    0.5|      5.99|         0.0|       47.09|\n",
      "|        1| 2024-01-02 10:42:58|  2024-01-02 10:48:58|              2|        13.03|      -73.984409|      40.751071|           1|                 N|       -74.003348|       40.739025|           2|      38.07|  0.0|    0.5|       0.0|         0.0|       38.57|\n",
      "|        2| 2024-01-22 19:21:59|  2024-01-22 19:59:59|              1|         8.51|      -73.966543|       40.82386|           1|                 N|       -73.956191|       40.810246|           1|      42.77|  0.0|    0.5|       2.3|         0.0|       45.57|\n",
      "+---------+--------------------+---------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Simple Data Loading Test\n",
    "print(\"📂 Loading data with local Spark...\")\n",
    "\n",
    "try:\n",
    "    # Very simple loading\n",
    "    taxi_df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(\"/home/jovyan/data/nyc_taxi_data.csv\")\n",
    "    \n",
    "    print(\"✅ Data loaded!\")\n",
    "    print(f\"Row count: {taxi_df.count()}\")\n",
    "    print(f\"Columns: {len(taxi_df.columns)}\")\n",
    "    \n",
    "    # Show first 3 rows\n",
    "    taxi_df.show(3)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe1f9def-0acb-49b8-978f-4f0f14fc0517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Data Quality Assessment:\n",
      "\n",
      "❓ Missing Values per Column:\n",
      "+---------+--------------------+---------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "|vendor_id|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|rate_code_id|store_and_fwd_flag|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+---------+--------------------+---------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "|        0|                   0|                    0|              0|            0|               0|              0|           0|                 0|                0|               0|           0|          0|    0|      0|         0|           0|           0|\n",
      "+---------+--------------------+---------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "\n",
      "\n",
      "📈 Numerical Columns Statistics:\n",
      "+-------+------------------+--------------------+---------------------+-----------------+-----------------+-------------------+-------------------+------------+------------------+-------------------+-------------------+------------------+------------------+-----+-------+------------------+------------+------------------+\n",
      "|summary|         vendor_id|tpep_pickup_datetime|tpep_dropoff_datetime|  passenger_count|    trip_distance|   pickup_longitude|    pickup_latitude|rate_code_id|store_and_fwd_flag|  dropoff_longitude|   dropoff_latitude|      payment_type|       fare_amount|extra|mta_tax|        tip_amount|tolls_amount|      total_amount|\n",
      "+-------+------------------+--------------------+---------------------+-----------------+-----------------+-------------------+-------------------+------------+------------------+-------------------+-------------------+------------------+------------------+-----+-------+------------------+------------+------------------+\n",
      "|  count|             10000|               10000|                10000|            10000|            10000|              10000|              10000|       10000|             10000|              10000|              10000|             10000|             10000|10000|  10000|             10000|       10000|             10000|\n",
      "|   mean|            1.4916|                NULL|                 NULL|           2.0115|10.17212199999997|  -73.9002803078999|  40.75043671140014|         1.0|              NULL| -73.90032195650019| 40.750533123799975|              1.25|43.230005999999875|  0.0|    0.5|4.0338960000000075|         0.0| 47.76390199999996|\n",
      "| stddev|0.4999544333672262|                NULL|                 NULL|1.069711259274119|5.711291076603632|0.08673974076472939|0.05755070294535415|         0.0|              NULL|0.08757643256480697|0.05874616947926592|0.4330343541512467|16.800345927459087|  0.0|    0.0| 4.128931257422052|         0.0|18.735807003190484|\n",
      "|    min|                 1| 2024-01-01 00:00:27|  2024-01-01 00:04:33|                1|              0.1|          -73.75007|           40.65003|           1|                 N|         -73.731878|          40.630564|                 1|              10.0|  0.0|    0.5|               0.0|         0.0|             10.05|\n",
      "|    max|                 2| 2024-01-31 23:48:41|  2024-02-01 00:24:25|                4|             9.99|         -74.049974|          40.849994|           1|                 N|          -74.06891|          40.868854|                 2|              9.97|  0.0|    0.5|              9.99|         0.0|             99.53|\n",
      "+-------+------------------+--------------------+---------------------+-----------------+-----------------+-------------------+-------------------+------------+------------------+-------------------+-------------------+------------------+------------------+-----+-------+------------------+------------+------------------+\n",
      "\n",
      "\n",
      "🗓 Date Range:\n",
      "+-------------------+-------------------+-----------+\n",
      "|    earliest_pickup|      latest_pickup|total_trips|\n",
      "+-------------------+-------------------+-----------+\n",
      "|2024-01-01 00:00:27|2024-01-31 23:48:41|      10000|\n",
      "+-------------------+-------------------+-----------+\n",
      "\n",
      "\n",
      "💳 Payment Type Distribution:\n",
      "+------------+-----+\n",
      "|payment_type|count|\n",
      "+------------+-----+\n",
      "|           1| 7500|\n",
      "|           2| 2500|\n",
      "+------------+-----+\n",
      "\n",
      "\n",
      "👥 Passenger Count Distribution:\n",
      "+---------------+-----+\n",
      "|passenger_count|count|\n",
      "+---------------+-----+\n",
      "|              1| 4220|\n",
      "|              2| 2889|\n",
      "|              3| 1447|\n",
      "|              4| 1444|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Quality Analysis\n",
    "print(\"🔍 Data Quality Assessment:\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n❓ Missing Values per Column:\")\n",
    "missing_counts = taxi_df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in taxi_df.columns\n",
    "])\n",
    "missing_counts.show()\n",
    "\n",
    "# Basic statistics for numerical columns\n",
    "print(\"\\n📈 Numerical Columns Statistics:\")\n",
    "taxi_df.describe().show()\n",
    "\n",
    "# Check data ranges\n",
    "print(\"\\n🗓 Date Range:\")\n",
    "taxi_df.select(\n",
    "    min(\"tpep_pickup_datetime\").alias(\"earliest_pickup\"),\n",
    "    max(\"tpep_pickup_datetime\").alias(\"latest_pickup\"),\n",
    "    count(\"*\").alias(\"total_trips\")\n",
    ").show()\n",
    "\n",
    "# Payment type distribution\n",
    "print(\"\\n💳 Payment Type Distribution:\")\n",
    "taxi_df.groupBy(\"payment_type\").count().orderBy(\"payment_type\").show()\n",
    "\n",
    "# Passenger count distribution\n",
    "print(\"\\n👥 Passenger Count Distribution:\")\n",
    "taxi_df.groupBy(\"passenger_count\").count().orderBy(\"passenger_count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18ec9101-3bb9-4d6e-a205-691bd299be13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Data Cleaning and Feature Engineering...\n",
      "📊 Original records: 10,000\n",
      "📊 After cleaning: 9,573\n",
      "📊 Removed: 427 records\n",
      "📊 Final dataset: 9,410 records\n",
      "💾 DataFrame cached for better performance\n",
      "\n",
      "✅ Data transformation completed!\n",
      "🔍 New columns: pickup_hour, pickup_day_of_week, trip_duration_minutes, tip_percentage, fare_per_mile\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Data Transformations and Feature Engineering\n",
    "print(\"🧹 Data Cleaning and Feature Engineering...\")\n",
    "\n",
    "# First, let's properly cast the datetime columns\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "# Convert string datetime to proper timestamp type\n",
    "taxi_df_typed = taxi_df.withColumn(\n",
    "    \"pickup_datetime\", to_timestamp(\"tpep_pickup_datetime\", \"yyyy-MM-dd HH:mm:ss\")\n",
    ").withColumn(\n",
    "    \"dropoff_datetime\", to_timestamp(\"tpep_dropoff_datetime\", \"yyyy-MM-dd HH:mm:ss\")\n",
    ")\n",
    "\n",
    "# Clean the data (remove invalid trips)\n",
    "cleaned_taxi_df = taxi_df_typed.filter(\n",
    "    (col(\"trip_distance\") > 0) &\n",
    "    (col(\"fare_amount\") > 0) &\n",
    "    (col(\"total_amount\") > 0) &\n",
    "    (col(\"passenger_count\") > 0) &\n",
    "    (col(\"passenger_count\") <= 6) &\n",
    "    (col(\"trip_distance\") < 50) &\n",
    "    (col(\"fare_amount\") < 200) &\n",
    "    (col(\"total_amount\") < 300)\n",
    ")\n",
    "\n",
    "print(f\"📊 Original records: {taxi_df.count():,}\")\n",
    "print(f\"📊 After cleaning: {cleaned_taxi_df.count():,}\")\n",
    "print(f\"📊 Removed: {taxi_df.count() - cleaned_taxi_df.count():,} records\")\n",
    "\n",
    "# Feature engineering\n",
    "enriched_taxi_df = cleaned_taxi_df.withColumn(\n",
    "    \"pickup_hour\", hour(\"pickup_datetime\")\n",
    ").withColumn(\n",
    "    \"pickup_day_of_week\", dayofweek(\"pickup_datetime\") \n",
    ").withColumn(\n",
    "    \"trip_duration_minutes\",\n",
    "    (unix_timestamp(\"dropoff_datetime\") - unix_timestamp(\"pickup_datetime\")) / 60\n",
    ").withColumn(\n",
    "    \"tip_percentage\",\n",
    "    when(col(\"fare_amount\") > 0, (col(\"tip_amount\") / col(\"fare_amount\")) * 100).otherwise(0)\n",
    ").withColumn(\n",
    "    \"fare_per_mile\",\n",
    "    when(col(\"trip_distance\") > 0, col(\"fare_amount\") / col(\"trip_distance\")).otherwise(0)\n",
    ")\n",
    "\n",
    "# Final filtering\n",
    "final_taxi_df = enriched_taxi_df.filter(\n",
    "    (col(\"trip_duration_minutes\") > 1) &\n",
    "    (col(\"trip_duration_minutes\") < 120) &\n",
    "    (col(\"fare_per_mile\") > 0) &\n",
    "    (col(\"fare_per_mile\") < 50)\n",
    ")\n",
    "\n",
    "print(f\"📊 Final dataset: {final_taxi_df.count():,} records\")\n",
    "\n",
    "# Cache for better performance\n",
    "final_taxi_df.cache()\n",
    "print(\"💾 DataFrame cached for better performance\")\n",
    "\n",
    "print(\"\\n✅ Data transformation completed!\")\n",
    "print(\"🔍 New columns: pickup_hour, pickup_day_of_week, trip_duration_minutes, tip_percentage, fare_per_mile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c40d19-ebd1-4d11-b1d6-e72a1f9d4aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Business Analytics with Spark SQL\n",
      "\n",
      "⏰ Peak Hours Analysis:\n",
      "+-----------+----------+------------+--------+-----------+\n",
      "|pickup_hour|trip_count|avg_distance|avg_fare|avg_tip_pct|\n",
      "+-----------+----------+------------+--------+-----------+\n",
      "|          0|       395|        10.7|   49.01|       9.37|\n",
      "|          1|       387|        10.5|   48.54|       9.15|\n",
      "|          2|       377|        10.8|   49.25|       9.37|\n",
      "|          3|       400|       10.29|   48.55|       9.88|\n",
      "|          4|       396|       10.69|   51.03|       9.04|\n",
      "|          5|       370|        10.5|   48.17|       8.91|\n",
      "|          6|       414|       10.78|   50.11|       9.29|\n",
      "|          7|       370|       10.01|   47.67|       9.41|\n",
      "|          8|       392|       10.96|   51.03|       9.65|\n",
      "|          9|       403|       10.59|   49.35|       9.07|\n",
      "|         10|       384|       10.81|    49.8|      10.12|\n",
      "|         11|       402|       10.46|    49.1|       8.91|\n",
      "|         12|       380|       10.77|   49.39|       8.66|\n",
      "|         13|       366|       10.04|   47.13|       9.31|\n",
      "|         14|       369|       10.64|   49.53|       9.28|\n",
      "|         15|       373|       11.07|   51.57|        9.8|\n",
      "|         16|       424|       10.86|   49.28|       9.31|\n",
      "|         17|       402|        10.9|   49.92|       9.58|\n",
      "|         18|       395|        10.5|   49.05|       9.88|\n",
      "|         19|       373|       10.25|   47.52|       9.48|\n",
      "|         20|       411|       10.42|   49.12|       9.42|\n",
      "|         21|       402|        10.6|   49.21|       9.02|\n",
      "|         22|       403|        10.8|    50.1|       9.78|\n",
      "|         23|       422|       10.42|   47.52|       9.16|\n",
      "+-----------+----------+------------+--------+-----------+\n",
      "\n",
      "\n",
      "📅 Day of Week Analysis:\n",
      "+---------+----------+------------+---------+\n",
      "| day_name|trip_count|avg_distance|avg_total|\n",
      "+---------+----------+------------+---------+\n",
      "|   Sunday|      1200|       10.58|    49.38|\n",
      "|   Monday|      1501|        10.6|    48.63|\n",
      "|  Tuesday|      1468|       10.57|    49.26|\n",
      "|Wednesday|      1528|       10.38|    48.78|\n",
      "| Thursday|      1220|       10.78|    49.35|\n",
      "|   Friday|      1262|       10.61|    49.79|\n",
      "| Saturday|      1231|       10.75|    49.52|\n",
      "+---------+----------+------------+---------+\n",
      "\n",
      "\n",
      "📏 Distance Analysis:\n",
      "+------------------+----------+----------------+--------+\n",
      "| distance_category|trip_count|avg_duration_min|avg_fare|\n",
      "+------------------+----------+----------------+--------+\n",
      "|     Short (≤1 mi)|       433|            30.5|   24.07|\n",
      "|   Medium (1-5 mi)|      1986|            30.5|   30.83|\n",
      "|    Long (5-10 mi)|      2465|            31.3|   43.62|\n",
      "|Very Long (>10 mi)|      4526|            31.3|   62.73|\n",
      "+------------------+----------+----------------+--------+\n",
      "\n",
      "\n",
      "🎯 Key Insights Generated!\n",
      "📊 You've successfully processed 10,000+ records with Spark!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Business Analytics with Spark SQL\n",
    "print(\"🎯 Business Analytics with Spark SQL\")\n",
    "\n",
    "# Register DataFrame as SQL table\n",
    "final_taxi_df.createOrReplaceTempView(\"taxi_trips\")\n",
    "\n",
    "# Analysis 1: Peak hours analysis\n",
    "print(\"\\n⏰ Peak Hours Analysis:\")\n",
    "peak_hours = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        pickup_hour,\n",
    "        COUNT(*) as trip_count,\n",
    "        ROUND(AVG(trip_distance), 2) as avg_distance,\n",
    "        ROUND(AVG(total_amount), 2) as avg_fare,\n",
    "        ROUND(AVG(tip_percentage), 2) as avg_tip_pct\n",
    "    FROM taxi_trips \n",
    "    GROUP BY pickup_hour \n",
    "    ORDER BY pickup_hour\n",
    "\"\"\")\n",
    "peak_hours.show(24)\n",
    "\n",
    "# Analysis 2: Day of week patterns  \n",
    "print(\"\\n📅 Day of Week Analysis:\")\n",
    "dow_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE pickup_day_of_week\n",
    "            WHEN 1 THEN 'Sunday'\n",
    "            WHEN 2 THEN 'Monday' \n",
    "            WHEN 3 THEN 'Tuesday'\n",
    "            WHEN 4 THEN 'Wednesday'\n",
    "            WHEN 5 THEN 'Thursday'\n",
    "            WHEN 6 THEN 'Friday'\n",
    "            WHEN 7 THEN 'Saturday'\n",
    "        END as day_name,\n",
    "        COUNT(*) as trip_count,\n",
    "        ROUND(AVG(trip_distance), 2) as avg_distance,\n",
    "        ROUND(AVG(total_amount), 2) as avg_total\n",
    "    FROM taxi_trips \n",
    "    GROUP BY pickup_day_of_week \n",
    "    ORDER BY pickup_day_of_week\n",
    "\"\"\")\n",
    "dow_analysis.show()\n",
    "\n",
    "# Analysis 3: Distance categories\n",
    "print(\"\\n📏 Distance Analysis:\")\n",
    "distance_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN trip_distance <= 1 THEN 'Short (≤1 mi)'\n",
    "            WHEN trip_distance <= 5 THEN 'Medium (1-5 mi)' \n",
    "            WHEN trip_distance <= 10 THEN 'Long (5-10 mi)'\n",
    "            ELSE 'Very Long (>10 mi)'\n",
    "        END as distance_category,\n",
    "        COUNT(*) as trip_count,\n",
    "        ROUND(AVG(trip_duration_minutes), 1) as avg_duration_min,\n",
    "        ROUND(AVG(total_amount), 2) as avg_fare\n",
    "    FROM taxi_trips\n",
    "    GROUP BY \n",
    "        CASE \n",
    "            WHEN trip_distance <= 1 THEN 'Short (≤1 mi)'\n",
    "            WHEN trip_distance <= 5 THEN 'Medium (1-5 mi)' \n",
    "            WHEN trip_distance <= 10 THEN 'Long (5-10 mi)'\n",
    "            ELSE 'Very Long (>10 mi)'\n",
    "        END\n",
    "    ORDER BY avg_duration_min\n",
    "\"\"\")\n",
    "distance_analysis.show()\n",
    "\n",
    "print(\"\\n🎯 Key Insights Generated!\")\n",
    "print(\"📊 You've successfully processed 10,000+ records with Spark!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "614d2d3b-3aa6-4a2f-84fc-244fb0ebdedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🗺 Geographic Analysis:\n",
      "🔝 Top Pickup Locations:\n",
      "+------------------+------------------+------------+-----------------+--------+\n",
      "|pickup_lon_rounded|pickup_lat_rounded|pickup_count|avg_trip_distance|avg_fare|\n",
      "+------------------+------------------+------------+-----------------+--------+\n",
      "+------------------+------------------+------------+-----------------+--------+\n",
      "\n",
      "\n",
      "✈ Airport Trip Analysis:\n",
      "+-----------+----------+------------------+------------------+------------------+\n",
      "|   location|trip_count|      avg_distance|          avg_fare|      avg_duration|\n",
      "+-----------+----------+------------------+------------------+------------------+\n",
      "|JFK Airport|        72|10.072083333333335|46.365555555555574|            28.625|\n",
      "|LGA Airport|       221|10.614615384615378|49.093755656108605|30.176470588235293|\n",
      "+-----------+----------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Geographic hot spots analysis\n",
    "print(\"\\n🗺 Geographic Analysis:\")\n",
    "\n",
    "# Popular pickup locations (rounded by coordinate)\n",
    "pickup_hotspots = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        ROUND(pickup_longitude, 3) AS pickup_lon_rounded,\n",
    "        ROUND(pickup_latitude, 3) AS pickup_lat_rounded,\n",
    "        COUNT(*) AS pickup_count,\n",
    "        AVG(trip_distance) AS avg_trip_distance,\n",
    "        AVG(total_amount) AS avg_fare\n",
    "    FROM taxi_trips\n",
    "    WHERE pickup_longitude BETWEEN -74.05 AND -73.75\n",
    "      AND pickup_latitude BETWEEN 40.65 AND 40.85\n",
    "    GROUP BY pickup_lon_rounded, pickup_lat_rounded\n",
    "    HAVING pickup_count >= 100\n",
    "    ORDER BY pickup_count DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "print(\"🔝 Top Pickup Locations:\")\n",
    "pickup_hotspots.show()\n",
    "\n",
    "# Airport trips analysis\n",
    "airport_trips = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        'JFK Airport' AS location,\n",
    "        COUNT(*) AS trip_count,\n",
    "        AVG(trip_distance) AS avg_distance,\n",
    "        AVG(total_amount) AS avg_fare,\n",
    "        AVG(trip_duration_minutes) AS avg_duration\n",
    "    FROM taxi_trips\n",
    "    WHERE (pickup_longitude BETWEEN -73.79 AND -73.76 AND pickup_latitude BETWEEN 40.64 AND 40.66)\n",
    "       OR (dropoff_longitude BETWEEN -73.79 AND -73.76 AND dropoff_latitude BETWEEN 40.64 AND 40.66)\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'LGA Airport' AS location,\n",
    "        COUNT(*) AS trip_count,\n",
    "        AVG(trip_distance) AS avg_distance,\n",
    "        AVG(total_amount) AS avg_fare,\n",
    "        AVG(trip_duration_minutes) AS avg_duration\n",
    "    FROM taxi_trips\n",
    "    WHERE (pickup_longitude BETWEEN -73.89 AND -73.85 AND pickup_latitude BETWEEN 40.76 AND 40.78)\n",
    "       OR (dropoff_longitude BETWEEN -73.89 AND -73.85 AND dropoff_latitude BETWEEN 40.76 AND 40.78)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✈ Airport Trip Analysis:\")\n",
    "airport_trips.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30829a8a-9424-4e23-b488-c9efaf8e8655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Performance Optimization Techniques\n",
      "\n",
      "📦 Current Partitions: 1\n",
      "📊 Records per Partition:\n",
      "+------------+---------------------+\n",
      "|partition_id|records_per_partition|\n",
      "+------------+---------------------+\n",
      "|           0|                 9410|\n",
      "+------------+---------------------+\n",
      "\n",
      "\n",
      "🔄 Optimizing Partitions...\n",
      "📦 New Partitions: 8\n",
      "\n",
      "📡 Broadcast Join Example:\n",
      "✅ Payment method mapping applied with broadcast join\n",
      "\n",
      "📈 Advanced Window Functions:\n",
      "✅ Window functions applied for running averages and rankings\n"
     ]
    }
   ],
   "source": [
    "# Performance optimization techniques\n",
    "print(\"🚀 Performance Optimization Techniques\")\n",
    "\n",
    "# 1. Partitioning analysis\n",
    "print(f\"\\n📦 Current Partitions: {final_taxi_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Check partition sizes\n",
    "partition_info = spark.sql(\"\"\"\n",
    "    SELECT spark_partition_id() AS partition_id, COUNT(*) AS records_per_partition\n",
    "    FROM taxi_trips\n",
    "    GROUP BY spark_partition_id()\n",
    "    ORDER BY spark_partition_id()\n",
    "\"\"\")\n",
    "\n",
    "print(\"📊 Records per Partition:\")\n",
    "partition_info.show()\n",
    "\n",
    "# 2. Repartitioning for better performance\n",
    "print(\"\\n🔄 Optimizing Partitions...\")\n",
    "# Repartition based on pickup_hour for time-based analysis\n",
    "optimized_df = final_taxi_df.repartition(8, \"pickup_hour\").cache()\n",
    "\n",
    "print(f\"📦 New Partitions: {optimized_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# 3. Broadcast join optimization example\n",
    "print(\"\\n📡 Broadcast Join Example:\")\n",
    "\n",
    "# Create a small lookup table for payment types\n",
    "payment_types = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"Credit Card\"),\n",
    "        (2, \"Cash\"),\n",
    "        (3, \"No Charge\"),\n",
    "        (4, \"Dispute\"),\n",
    "        (5, \"Unknown\"),\n",
    "        (6, \"Voided Trip\")\n",
    "    ],\n",
    "    [\"payment_type\", \"payment_method\"]\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "enriched_df = optimized_df.join(\n",
    "    broadcast(payment_types),\n",
    "    on=\"payment_type\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"✅ Payment method mapping applied with broadcast join\")\n",
    "\n",
    "# 4. Aggregation with window functions\n",
    "print(\"\\n📈 Advanced Window Functions:\")\n",
    "\n",
    "from pyspark.sql.functions import avg, row_number, desc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Calculate running averages and rankings\n",
    "window_spec = Window.partitionBy(\"pickup_hour\").orderBy(\"tpep_pickup_datetime\")\n",
    "\n",
    "windowed_analysis = (\n",
    "    optimized_df\n",
    "    .withColumn(\n",
    "        \"running_avg_fare\",\n",
    "        avg(\"total_amount\").over(window_spec.rowsBetween(-100, 0))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"trip_rank_in_hour\",\n",
    "        row_number().over(Window.partitionBy(\"pickup_hour\").orderBy(desc(\"total_amount\")))\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"✅ Window functions applied for running averages and rankings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6a1fb5-bf0c-4238-9ff0-468fbf9e3547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
